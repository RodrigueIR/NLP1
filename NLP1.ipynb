{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPOrGbQ6C1JEkIo8qL/pz2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RodrigueIR/NLP1/blob/main/NLP1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lw1hV6MWTH0y",
        "outputId": "0187882f-6e3d-4b01-894e-9c5479ec4df4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a sample text 2pak\n"
          ]
        }
      ],
      "source": [
        "# Removing HTML Tags & Special characters\n",
        "import re # Regular expression library\n",
        "\n",
        "text = \"<p> This  is a <b> sample</b> text @2pak! </p>\"\n",
        "clean_text = re.sub(r'<.*?>','', text) #Remove HTML tags\n",
        "\n",
        "clean_text = re.sub(r'[^a-zA-Z0-9\\s]','', clean_text) #Removes non-alphabetic chars\n",
        "clean_text = re.sub(r'\\s+',' ', clean_text).strip() # Removes extra spaces\n",
        "print(clean_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization"
      ],
      "metadata": {
        "id": "NsnJNj0xVi7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "text = \"This is my first class in ML. Machine learning is fascinating!\"\n",
        "tokens = sent_tokenize(text)\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6k4yNzYSVmwE",
        "outputId": "984ba7b0-41c5-427f-e5c6-6f0ac891d364"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This is my first class in ML.', 'Machine learning is fascinating!']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(text)\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRepoH0jWs8r",
        "outputId": "6663424a-29ef-4102-dc17-61380833b5f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This',\n",
              " 'is',\n",
              " 'my',\n",
              " 'first',\n",
              " 'class',\n",
              " 'in',\n",
              " 'ML',\n",
              " '.',\n",
              " 'Machine',\n",
              " 'learning',\n",
              " 'is',\n",
              " 'fascinating',\n",
              " '!']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normalization**"
      ],
      "metadata": {
        "id": "gVavXKobXSme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Text Normalization\"\n",
        "normalized_text = text.lower()\n",
        "normalized_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "4F1t80RlXaXp",
        "outputId": "0d2e52e0-f61e-4198-c066-e9d391ee1ec3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'text normalization'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatization**"
      ],
      "metadata": {
        "id": "ZivQ3kxFX0oP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# initialize stemmer and lemmatizer\n",
        "stemmer =  PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Sample sentence\n",
        "sentence = \"The cats were running and jumping happily.\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "words = word_tokenize(sentence)\n",
        "\n",
        "# Stemming: Reduces words to their root foprm (crude but fast).\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "# lemmatization: Uses vocabulary to convert words to base form (more accurate).\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
        "\n",
        "# print the results\n",
        "print(\"Original words:\", words)\n",
        "print(\"Stemmed words:\", stemmed_words)\n",
        "print(\"Lemmatized words:\", lemmatized_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocoX5Q0-Xtuo",
        "outputId": "3385c13b-f895-4f59-a6ad-67f7e0353f8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words: ['The', 'cats', 'were', 'running', 'and', 'jumping', 'happily', '.']\n",
            "Stemmed words: ['the', 'cat', 'were', 'run', 'and', 'jump', 'happili', '.']\n",
            "Lemmatized words: ['The', 'cat', 'be', 'run', 'and', 'jump', 'happily', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stop Word Removal**"
      ],
      "metadata": {
        "id": "uJer2Hqsa0Qj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "# Print (stopwords)\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "filtered_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cu2yEpG1a-oh",
        "outputId": "9fc62232-aa52-4dd3-8823-d7e668c356c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['first', 'class', 'ML', '.', 'Machine', 'learning', 'fascinating', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handling Misspellings & Typos**"
      ],
      "metadata": {
        "id": "sOGR93DfcJ5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "text = \"I havv a speling mistake.\"\n",
        "\n",
        "corrected = TextBlob(text).correct()\n",
        "corrected"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6UkdQAjcStf",
        "outputId": "fbc2e340-e9a9-4f68-d34e-e64db8e6612f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextBlob(\"I have a spelling mistake.\")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exploratory Data Analysis (EDA)**"
      ],
      "metadata": {
        "id": "UYpxw21idlze"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0bZQG3gLdtnl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}